# SOME DESCRIPTIVE TITLE.
# Copyright (C) Doug Hellmann
# This file is distributed under the same license as the Python Module of the Week package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Python Module of the Week 2.1.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2015-10-06 02:58-0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../PyMOTW/robotparser/index.rst:3
# db4080afa8e0486b9577e8a1fb84ed0f
msgid "robotparser -- Internet spider access control"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:8
# 3676af52539f4ebcbf8dc6eac4b50633
msgid "Parse robots.txt file used to control Internet spiders"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:9
# 8c29fd935cb045d8996b788df9a36d67
msgid "2.1.3 and later"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:11
# be604a31bfed47819fa1d4c3009c76a0
msgid ":mod:`robotparser` implements a parser for the ``robots.txt`` file format, including a simple function for checking if a given user agent can access a resource.  It is intended for use in well-behaved spiders or other crawler applications that need to either be throttled or otherwise restricted."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:15
# b882e02f5e3a497f948c843e0007ea53
msgid "The :mod:`robotparser` module has been renamed :mod:`urllib.robotparser` in Python 3.0. Existing code using :mod:`robotparser` can be updated using 2to3."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:19
# a4d29c3c8b564d97a9af05160eeb7b1d
msgid "robots.txt"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:21
# 98bd7f9d459448768a984c3191ecc3a4
msgid "The ``robots.txt`` file format is a simple text-based access control system for computer programs that automatically access web resources (\"spiders\", \"crawlers\", etc.).  The file is made up of records that specify the user agent identifier for the program followed by a list of URLs (or URL prefixes) the agent may not access."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:23
# 320574ed76b14922a953914f9066a669
msgid "This is the ``robots.txt`` file for ``http://www.doughellmann.com/``:"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:28
# 2e5bd9d90fa44f6f92f26a155d6b596b
msgid "It prevents access to some of the expensive parts of my site that would overload the server if a search engine tried to index them.  For a more complete set of examples, refer to `The Web Robots Page`_."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:31
# 8788cc2690b3410fa664d1156d9ae2c4
msgid "Simple Example"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:33
# e4bd3665d4e24936bb36c268ac47609f
msgid "Using the data above, a simple crawler can test whether it is allowed to download a page using the ``RobotFileParser``'s ``can_fetch()`` method."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:39
# 7affdbff476e434ab348ae3109fe7b53
msgid "The URL argument to ``can_fetch()`` can be a path relative to the root of the site, or full URL."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:67
# 99e3d58244af4e939dd3a717ff048056
msgid "Long-lived Spiders"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:69
# 5e21937bfe5e44fe80a9006ff1f0aad9
msgid "An application that takes a long time to process the resources it downloads or that is throttled to pause between downloads may want to check for new ``robots.txt`` files periodically based on the age of the content it has downloaded already.  The age is not managed automatically, but there are convenience methods to make tracking it easier."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:75
# bc1c2ab925f54ca18cf2caed245bd0ba
msgid "This extreme example downloads a new ``robots.txt`` file if the one it has is more than 1 second old."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:100
# d5004938a7b946f0982076fdc001b7e2
msgid "A \"nicer\" version of the long-lived application might request the modification time for the file before downloading the entire thing.  On the other hand, ``robots.txt`` files are usually fairly small, so it isn't that much more expensive to just grab the entire document again."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:106
# 78065c28b7b043b3bd3fa6b811c24dd5
msgid "`robotparser <http://docs.python.org/library/robotparser.html>`_"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:106
# 5a16fd552d324b24a670681293059dc0
msgid "The standard library documentation for this module."
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:108
# e98fa82cdff2422e81231c90888bb4af
msgid "`The Web Robots Page`_"
msgstr ""

#: ../../PyMOTW/robotparser/index.rst:109
# c804412490874954b1e9c9e360259069
msgid "Description of robots.txt format."
msgstr ""

